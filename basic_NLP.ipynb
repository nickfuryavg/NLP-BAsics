{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickfuryavg/NLP-BAsics/blob/main/basic_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d08fa3ca",
      "metadata": {
        "id": "d08fa3ca",
        "outputId": "62d00877-9994-4256-d76c-1e8212934c39"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      One of the other reviewers has mentioned that ...  positive\n",
              "1      A wonderful little production. <br /><br />The...  positive\n",
              "2      I thought this was a wonderful way to spend ti...  positive\n",
              "3      Basically there's a family where a little boy ...  negative\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "...                                                  ...       ...\n",
              "49995  I thought this movie did a down right good job...  positive\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
              "49997  I am a Catholic taught in parochial elementary...  negative\n",
              "49998  I'm going to have to disagree with the previou...  negative\n",
              "49999  No one expects the Star Trek movies to be high...  negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df=pd.read_csv(\"IMDB Dataset.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc2b78c7",
      "metadata": {
        "id": "cc2b78c7",
        "outputId": "e83ab3b8-2a8e-4fa5-8fb0-a750a1e0c19e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['review'][3].lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed3f120d",
      "metadata": {
        "id": "ed3f120d"
      },
      "source": [
        "## converting the whole dataset to lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e340f563",
      "metadata": {
        "id": "e340f563"
      },
      "outputs": [],
      "source": [
        "df['review']=df['review'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd631bf4",
      "metadata": {
        "id": "dd631bf4",
        "outputId": "aec0d616-cb19-4233-db2b-e56524026dad"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  one of the other reviewers has mentioned that ...  positive\n",
              "1  a wonderful little production. <br /><br />the...  positive\n",
              "2  i thought this was a wonderful way to spend ti...  positive\n",
              "3  basically there's a family where a little boy ...  negative\n",
              "4  petter mattei's \"love in the time of money\" is...  positive"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c79ed249",
      "metadata": {
        "id": "c79ed249"
      },
      "source": [
        "## removing html tags from a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efbf92ee",
      "metadata": {
        "id": "efbf92ee"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    \"\"\"Remove HTML tags from a string.\"\"\"\n",
        "    pattern = re.compile('<.*?>')\n",
        "    return pattern.sub( r'', text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30754f4a",
      "metadata": {
        "id": "30754f4a"
      },
      "outputs": [],
      "source": [
        "df['review']=df['review'].apply(remove_html_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "519081cc",
      "metadata": {
        "id": "519081cc",
        "outputId": "34bd34be-4a75-4f31-89ff-afa9183d0e64"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a wonderful little production. the filming tec...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>i thought this movie did a down right good job...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>i am a catholic taught in parochial elementary...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>i'm going to have to disagree with the previou...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>no one expects the star trek movies to be high...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      one of the other reviewers has mentioned that ...  positive\n",
              "1      a wonderful little production. the filming tec...  positive\n",
              "2      i thought this was a wonderful way to spend ti...  positive\n",
              "3      basically there's a family where a little boy ...  negative\n",
              "4      petter mattei's \"love in the time of money\" is...  positive\n",
              "...                                                  ...       ...\n",
              "49995  i thought this movie did a down right good job...  positive\n",
              "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
              "49997  i am a catholic taught in parochial elementary...  negative\n",
              "49998  i'm going to have to disagree with the previou...  negative\n",
              "49999  no one expects the star trek movies to be high...  negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ebb0ce3",
      "metadata": {
        "id": "8ebb0ce3"
      },
      "source": [
        "## removing URL from a text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0140851",
      "metadata": {
        "id": "b0140851",
        "outputId": "811187b5-4598-4251-c701-3df7cd8daf6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Check out this website:  also visit \n",
            "Check out this website also visit \n",
            "jai shree ram  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def remove_urls(text):\n",
        "    \"\"\"Remove URLs from a string.\"\"\"\n",
        "    # Regular expression pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    # Replace URLs with an empty string\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "# Example usage:\n",
        "text_with_urls1 = \"Check out this website: https://example.com, also visit www.google.com\"\n",
        "text_with_urls2 = \"Check out this website also visit www.google.com\"\n",
        "text_with_urls3 = \"jai shree ram http://localhost:8888/notebooks/PyTorch%20Basics/natural%20Language%20Processing%20/Untitled.ipynb#removing-html-tags \"\n",
        "\n",
        "clean_text1 = remove_urls(text_with_urls1)\n",
        "clean_text2 = remove_urls(text_with_urls2)\n",
        "clean_text3 = remove_urls(text_with_urls3)\n",
        "\n",
        "print(clean_text1)\n",
        "print(clean_text2)\n",
        "print(clean_text3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86483d8a",
      "metadata": {
        "id": "86483d8a"
      },
      "source": [
        "## removing puntuation from a particular column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c91e751c",
      "metadata": {
        "id": "c91e751c",
        "outputId": "7d94bd37-0a8b-405a-b135-fb530e854636"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'basically theres a family where a little boy jake thinks theres a zombie in his closet  his parents are fighting all the timethis movie is slower than a soap opera and suddenly jake decides to become rambo and kill the zombieok first of all when youre going to make a film you must decide if its a thriller or a drama as a drama the movie is watchable parents are divorcing  arguing like in real life and then we have jake with his closet which totally ruins all the film i expected to see a boogeyman similar movie and instead i watched a drama with some meaningless thriller spots3 out of 10 just for the well playing parents  descent dialogs as for the shots with jake just ignore them'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Remove punctuation from a string.\"\"\"\n",
        "    # Regular expression pattern to match punctuation\n",
        "    punctuation_pattern = re.compile(r'[^\\w\\s]')\n",
        "    # Replace punctuation with an empty string\n",
        "    return punctuation_pattern.sub('', text)\n",
        "\n",
        "# Apply the remove_punctuation function to the 'review' column\n",
        "df['review'] = df['review'].apply(remove_punctuation)\n",
        "\n",
        "# Print the updated dataset\n",
        "df['review'][3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26d0cb44",
      "metadata": {
        "id": "26d0cb44"
      },
      "source": [
        "## removing puntuation from a particular column in another dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3488ea",
      "metadata": {
        "id": "3d3488ea",
        "outputId": "ea5f377f-2b77-4672-9ec2-a3ad80917b04"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label                                              tweet\n",
              "0   1      0   @user when a father is dysfunctional and is s...\n",
              "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
              "2   3      0                                bihday your majesty\n",
              "3   4      0  #model   i love u take with u all the time in ...\n",
              "4   5      0             factsguide: society now    #motivation"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1=pd.read_csv('train.csv')\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5817f5",
      "metadata": {
        "id": "fe5817f5",
        "outputId": "aca05d05-da6c-4e94-b199-cd6143211dd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'  bihday your majesty'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1['tweet'][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1fe2c7a",
      "metadata": {
        "id": "f1fe2c7a",
        "outputId": "247cb7ed-8d03-42bc-a3aa-acabb3f90d7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'model   i love u take with u all the time in ur√∞ √∞√∞√∞√∞\\x85√∞√∞√∞  '"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def remove_punctuation1(text):\n",
        "    \"\"\"Remove punctuation from a string.\"\"\"\n",
        "    # Regular expression pattern to match punctuation\n",
        "    punctuation_pattern = re.compile(r'[^\\w\\s]')\n",
        "    # Replace punctuation with an empty string\n",
        "    return punctuation_pattern.sub('', text)\n",
        "\n",
        "# Apply the remove_punctuation function to the 'review' column\n",
        "df1['tweet'] = df1['tweet'].apply(remove_punctuation)\n",
        "\n",
        "# Print the updated dataset\n",
        "df1['tweet'][3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e3e749",
      "metadata": {
        "id": "e2e3e749"
      },
      "source": [
        "## chat word treatement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f1391c0",
      "metadata": {
        "id": "4f1391c0",
        "outputId": "8b4ceaf6-962b-42ee-9ff6-868f9dc0fa66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "you only live once I don't know what to say by the way\n"
          ]
        }
      ],
      "source": [
        "chat_words_dict = {\n",
        "    \"lol\": \"laughing out loud\",\n",
        "    \"brb\": \"be right back\",\n",
        "    \"idk\": \"I don't know\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"afk\": \"away from keyboard\",\n",
        "    \"imho\": \"in my humble opinion\",\n",
        "    \"b4\": \"before\",\n",
        "    \"np\": \"no problem\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"yw\": \"you're welcome\",\n",
        "    \"ttyl\": \"talk to you later\",\n",
        "    \"gtg\": \"got to go\",\n",
        "    \"np\": \"no problem\",\n",
        "    \"tbh\": \"to be honest\",\n",
        "    \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"stfu\": \"shut the f*ck up\",\n",
        "    \"wtf\": \"what the f*ck\",\n",
        "    \"gg\": \"good game\",\n",
        "    \"rip\": \"rest in peace\",\n",
        "    \"nvm\": \"never mind\",\n",
        "    \"smh\": \"shaking my head\",\n",
        "    \"ily\": \"I love you\",\n",
        "    \"tmi\": \"too much information\",\n",
        "    \"fomo\": \"fear of missing out\",\n",
        "    \"bbl\": \"be back later\",\n",
        "    \"fml\": \"f*ck my life\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"icymi\": \"in case you missed it\",\n",
        "    \"idc\": \"I don't care\",\n",
        "    \"hmu\": \"hit me up\",\n",
        "    \"ic\": \"I see\",\n",
        "    \"jk\": \"just kidding\",\n",
        "    \"np\": \"no problem\",\n",
        "    \"nbd\": \"no big deal\",\n",
        "    \"af\": \"as f*ck\",\n",
        "    \"smh\": \"shaking my head\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"afaik\": \"as far as I know\",\n",
        "    \"gtfo\": \"get the f*ck out\",\n",
        "    \"hbd\": \"happy birthday\",\n",
        "    \"tgif\": \"thank god it's Friday\",\n",
        "    \"oomf\": \"one of my friends/followers\",\n",
        "    \"yolo\": \"you only live once\",\n",
        "    \"nsfw\": \"not safe for work\"\n",
        "    # Add more chat words and their expansions as needed\n",
        "}\n",
        "\n",
        "def treat_chat_words(text):\n",
        "    \"\"\"Expand chat words in a text.\"\"\"\n",
        "    words = text.split()\n",
        "    treated_words = []\n",
        "    for word in words:\n",
        "        treated_word = chat_words_dict.get(word.lower(), word)\n",
        "        treated_words.append(treated_word)\n",
        "    return ' '.join(treated_words)\n",
        "\n",
        "# Example usage:\n",
        "text = \"yolo idk what to say btw\"\n",
        "treated_text = treat_chat_words(text)\n",
        "print(treated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fe05caa",
      "metadata": {
        "id": "6fe05caa"
      },
      "source": [
        "## spelling correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b54ef4f8",
      "metadata": {
        "id": "b54ef4f8",
        "outputId": "4a8ddebe-764e-4929-f459-460feeaeb94f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textblob\n",
            "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nltk>=3.8\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (4.64.1)\n",
            "Requirement already satisfied: joblib in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (1.3.2)\n",
            "Requirement already satisfied: click in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
            "Installing collected packages: nltk, textblob\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed nltk-3.8.1 textblob-0.18.0.post0\n",
            "Original text: Ths is an exmple of text with speling miskates.\n",
            "Corrected text: The is an example of text with spelling mistakes.\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "def correct_spelling(text):\n",
        "    \"\"\"Correct spelling in a text using TextBlob.\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    corrected_text = blob.correct()\n",
        "    return str(corrected_text)\n",
        "\n",
        "# Example usage:\n",
        "text = \"Ths is an exmple of text with speling miskates.\"\n",
        "corrected_text = correct_spelling(text)\n",
        "print(\"Original text:\", text)\n",
        "print(\"Corrected text:\", corrected_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1a0f178",
      "metadata": {
        "id": "e1a0f178"
      },
      "source": [
        "## removing stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "626e9630",
      "metadata": {
        "id": "626e9630",
        "outputId": "ed8131d4-f9ae-4f61-895d-af1629300259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (3.8.1)\r\n",
            "Requirement already satisfied: tqdm in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from nltk) (4.64.1)\r\n",
            "Requirement already satisfied: joblib in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from nltk) (1.3.2)\r\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from nltk) (2022.7.9)\r\n",
            "Requirement already satisfied: click in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from nltk) (8.0.4)\r\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c35644",
      "metadata": {
        "id": "c4c35644"
      },
      "source": [
        "## showing all the stopwords in english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34a008ab",
      "metadata": {
        "id": "34a008ab",
        "outputId": "59b99159-2ab2-4ad4-f70b-cbaa560eab20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'for', 'nor', 'will', 'being', 'shouldn', \"weren't\", 'which', 'who', 'to', 'itself', 'during', 'don', 'all', 'most', 'our', 'having', 'that', 'more', 'not', 'shan', 'isn', 'if', 'm', \"wasn't\", 'because', 'haven', 'there', 'each', 'both', 'had', 'then', \"doesn't\", \"you're\", 'as', 'yourself', 'out', 're', 'hers', \"couldn't\", 'ours', 'am', 'very', 'those', 'why', \"hadn't\", 'an', 'wouldn', 'needn', \"mightn't\", 'this', 'only', 'once', 'until', 'yourselves', 'or', 'some', 'himself', 'have', 'myself', 'was', 'again', 'aren', 'ma', 'any', 'can', 'him', 'so', 'won', 'at', 'ourselves', 'couldn', 'but', 'me', 'than', 'been', 'off', 'now', 'doesn', 'her', 'too', \"mustn't\", 'she', \"isn't\", 'other', 'o', 'i', 'did', 'own', 'over', \"should've\", 'does', \"it's\", 'few', 'weren', 'the', 'through', \"wouldn't\", \"aren't\", \"won't\", 'further', 'wasn', 'above', 'up', 'here', 'doing', 'while', 'we', \"you've\", 'after', 'll', 'them', 't', 'and', 'hasn', 'it', 'just', \"didn't\", 'into', 'he', 'between', 'its', \"don't\", 'under', 'these', 'a', \"haven't\", 'his', 'my', 'down', \"you'd\", 'didn', \"shan't\", 'before', 'be', 'has', 'by', 'theirs', 'when', 'hadn', 'about', 'below', 'were', 'whom', 'ain', 'are', 'such', 'themselves', 'mightn', \"shouldn't\", 'same', \"she's\", 'what', 'on', 've', 'do', 'mustn', \"needn't\", 'their', \"hasn't\", 'where', 'no', 'herself', 'with', 'yours', 'is', 'y', 'you', 's', 'against', 'should', 'how', 'your', 'they', 'from', 'in', 'of', 'd', \"you'll\", \"that'll\"}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sandipanray/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the list of stopwords (only need to run this once)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Print all the stopwords\n",
        "print(stop_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e0a868e",
      "metadata": {
        "id": "2e0a868e"
      },
      "source": [
        "## removing stopwords from a single text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17949b89",
      "metadata": {
        "id": "17949b89",
        "outputId": "c292d2ae-8cbd-42dd-d04f-96ec025de4b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Sentence: This is a sample sentence and  fuck off, showing off the stop words filtration.\n",
            "Filtered Sentence: sample sentence fuck , showing stop words filtration .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sandipanray/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sandipanray/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the list of stopwords (only need to run this once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def remove_stopwords(sentence):\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(sentence)\n",
        "    # Get the list of English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Filter out stopwords from the sentence\n",
        "    filtered_sentence = [word for word in words if word.lower() not in stop_words]\n",
        "    # Join the words back into a sentence\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "# Example usage:\n",
        "sentence = \"This is a sample sentence and  fuck off, showing off the stop words filtration.\"\n",
        "filtered_sentence = remove_stopwords(sentence)\n",
        "print(\"Original Sentence:\", sentence)\n",
        "print(\"Filtered Sentence:\", filtered_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9b6455f",
      "metadata": {
        "id": "d9b6455f"
      },
      "source": [
        "##  removing stopwords from a particular column in dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0921e95a",
      "metadata": {
        "id": "0921e95a",
        "outputId": "beb0966a-b295-481c-8c42-ae45e0bace81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sandipanray/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sandipanray/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                  review sentiment\n",
            "0      one reviewers mentioned watching 1 oz episode ...  positive\n",
            "1      wonderful little production filming technique ...  positive\n",
            "2      thought wonderful way spend time hot summer we...  positive\n",
            "3      basically theres family little boy jake thinks...  negative\n",
            "4      petter matteis love time money visually stunni...  positive\n",
            "...                                                  ...       ...\n",
            "49995  thought movie right good job wasnt creative or...  positive\n",
            "49996  bad plot bad dialogue bad acting idiotic direc...  negative\n",
            "49997  catholic taught parochial elementary schools n...  negative\n",
            "49998  im going disagree previous comment side maltin...  negative\n",
            "49999  one expects star trek movies high art fans exp...  negative\n",
            "\n",
            "[50000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the list of stopwords (only need to run this once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def remove_stopwords(sentence):\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(sentence)\n",
        "    # Get the list of English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Filter out stopwords from the sentence\n",
        "    filtered_sentence = [word for word in words if word.lower() not in stop_words]\n",
        "    # Join the words back into a sentence\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "\n",
        "\n",
        "# Apply remove_stopwords function to the 'text' column\n",
        "df['review'] = df['review'].apply(remove_stopwords)\n",
        "\n",
        "# Print the updated dataset\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950a029b",
      "metadata": {
        "id": "950a029b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e6313d02",
      "metadata": {
        "id": "e6313d02"
      },
      "source": [
        "## removing emojis from a text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a89cbf7",
      "metadata": {
        "id": "0a89cbf7",
        "outputId": "f86050af-fcdd-4ab0-ddc7-8669b78af46e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: Hello! üòä How are you today? üåü\n",
            "Text without emojis: Hello!  How are you today? \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def remove_emojis(text):\n",
        "    \"\"\"Remove emojis from a text.\"\"\"\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Example usage:\n",
        "text_with_emojis = \"Hello! üòä How are you today? üåü\"\n",
        "text_without_emojis = remove_emojis(text_with_emojis)\n",
        "print(\"Original text:\", text_with_emojis)\n",
        "print(\"Text without emojis:\", text_without_emojis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5e82392",
      "metadata": {
        "id": "b5e82392"
      },
      "source": [
        "## decoding emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d537933f",
      "metadata": {
        "id": "d537933f",
        "outputId": "2391e94e-7bdf-4db7-9ffa-dbeac4d1d422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2936d46e",
      "metadata": {
        "id": "2936d46e",
        "outputId": "ae252f58-f54a-4797-ffa4-fd47b8b7d1b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: Hello! üôç‚Äç‚ôÄÔ∏è How are you today? üåü\n",
            "Text with emoji descriptions: Hello! :woman_frowning: How are you today? :glowing_star:\n"
          ]
        }
      ],
      "source": [
        "import emoji\n",
        "\n",
        "def demojize_text(text):\n",
        "    \"\"\"Convert emojis in a text into their textual representations.\"\"\"\n",
        "    return emoji.demojize(text)\n",
        "\n",
        "# Example usage:\n",
        "text_with_emojis = \"Hello! üôç‚Äç‚ôÄÔ∏è How are you today? üåü\"\n",
        "text_with_descriptions = demojize_text(text_with_emojis)\n",
        "print(\"Original text:\", text_with_emojis)\n",
        "print(\"Text with emoji descriptions:\", text_with_descriptions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41bd84a7",
      "metadata": {
        "id": "41bd84a7"
      },
      "source": [
        "## sentence tokenization and word tokenization using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a709a6e9",
      "metadata": {
        "id": "a709a6e9",
        "outputId": "bf54aefd-0e6b-4aee-dc1e-7edaf4e0fdc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word tokens: ['This', ',', 'is', 'a', 'test', '!', 'How', \"'s\", 'it', 'going', '?', 'I', \"'m\", 'doing', 'well', ',', 'thank', 'you', '!', '!', '.', 'I', 'have', 'a', 'Ph.d', 'IN', 'A.I', '.']\n",
            "sentance tokens: ['This, is a test!', \"How's it going?\", \"I'm doing well, thank you!!.\", 'I have a Ph.d IN A.I.']\n"
          ]
        }
      ],
      "source": [
        "##Tokenization is the process of breaking down a text into smaller units, such as words, phrases, symbols, or other meaningful elements. These smaller units are called tokens. Tokenization is an essential step in natural language processing (NLP) task\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Sample text with complex punctuation\n",
        "text = \"This, is a test! How's it going? I'm doing well, thank you!!. I have a Ph.d IN A.I.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "word_tokens = word_tokenize(text)\n",
        "sent_tokens = sent_tokenize(text)\n",
        "\n",
        "# Print the word tokens\n",
        "print(\"Word tokens:\", word_tokens)\n",
        "print(\"sentance tokens:\",sent_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d78e3fa",
      "metadata": {
        "id": "2d78e3fa"
      },
      "source": [
        "## tokenization using spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "278798ee",
      "metadata": {
        "id": "278798ee",
        "outputId": "de01347c-8189-471d-cc82-d4f008bca37f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.7.4-cp310-cp310-macosx_11_0_arm64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.8-cp310-cp310-macosx_11_0_arm64.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.9-cp310-cp310-macosx_11_0_arm64.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
            "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
            "Requirement already satisfied: jinja2 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy) (22.0)\n",
            "Collecting thinc<8.3.0,>=8.2.2\n",
            "  Downloading thinc-8.2.3-cp310-cp310-macosx_11_0_arm64.whl (789 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m789.6/789.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy) (1.23.5)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer<0.10.0,>=0.3.0\n",
            "  Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.8-cp310-cp310-macosx_11_0_arm64.whl (491 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting weasel<0.4.0,>=0.1.0\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
            "  Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy) (65.6.3)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.10-cp310-cp310-macosx_11_0_arm64.whl (26 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting language-data>=1.2\n",
            "  Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.18.2\n",
            "  Downloading pydantic_core-2.18.2-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.6.1\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Collecting annotated-types>=0.4.0\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.11-cp310-cp310-macosx_11_0_arm64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
            "Collecting marisa-trie>=0.7.7\n",
            "  Downloading marisa_trie-1.1.1-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m174.9/174.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cymem, wasabi, typing-extensions, spacy-loggers, spacy-legacy, murmurhash, marisa-trie, catalogue, blis, annotated-types, typer, srsly, pydantic-core, preshed, language-data, cloudpathlib, pydantic, langcodes, confection, weasel, thinc, spacy\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.1.1 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.7.1 pydantic-core-2.18.2 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 typing-extensions-4.11.0 wasabi-1.1.2 weasel-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ebd6d2",
      "metadata": {
        "id": "48ebd6d2",
        "outputId": "4b471e7f-4c5c-4aca-f1ed-9d70f3c5d2c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.1)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.6.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (22.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: language-data>=1.2 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/sandipanray/anaconda3/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "332ad7ff",
      "metadata": {
        "id": "332ad7ff",
        "outputId": "8be088eb-6ca4-4c0f-855f-e7d4e7d707da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The\n",
            "quick\n",
            "brown\n",
            "fox\n",
            "jumps\n",
            "over\n",
            "the\n",
            "lazy\n",
            "dog\n",
            "sandipan.ray98@gmail.com\n",
            "I\n",
            "love\n",
            "coding\n",
            "and\n",
            "solving\n",
            "challenging\n",
            "problems\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sent1=\"The quick brown fox jumps over the lazy dog sandipan.ray98@gmail.com\"\n",
        "sent2=\"I love coding and solving challenging problems.\"\n",
        "sent3= \"She sells seashells by the seashore.\"\n",
        "sent4=\"The sun sets in the west, painting the sky with vibrant colors.\"\n",
        "sent5=\"Tomorrow's weather forecast predicts scattered showers in the afternoon.\"\n",
        "\n",
        "# converting the sentance to documents\n",
        "doc1=nlp(sent1)\n",
        "doc2=nlp(sent2)\n",
        "doc3=nlp(sent3)\n",
        "doc4=nlp(sent4)\n",
        "doc5=nlp(sent5)\n",
        "\n",
        "for token1 in doc1:\n",
        "    print(token1)\n",
        "\n",
        "for token2 in doc2:\n",
        "    print(token2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e5e530e",
      "metadata": {
        "id": "9e5e530e"
      },
      "source": [
        "## stemming using porterstemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "869d3d0a",
      "metadata": {
        "id": "869d3d0a",
        "outputId": "9627b5af-5e78-4831-dbd5-cb84a40f6dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in the realm of artificial intelligence, researchers are exploring cutting-edge techniques such as deep learning and natural language processing to develop advanced systems capable of understanding and generating human-like speech, enabling machines to interact seamlessly with users across diverse domains, from healthcare and finance to entertainment and education, thereby revolutionizing the way we work, learn, and communicate in the digital ag\n"
          ]
        }
      ],
      "source": [
        "## Stemming is a text normalization technique used in natural language processing (NLP) to reduce words to their root or base form. The root form may not necessarily be a valid word in the language but is often the morphological root.\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Create a Porter stemmer object\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Example words to be stemmed\n",
        "text = \"In the realm of artificial intelligence, researchers are exploring cutting-edge techniques such as deep learning and natural language processing to develop advanced systems capable of understanding and generating human-like speech, enabling machines to interact seamlessly with users across diverse domains, from healthcare and finance to entertainment and education, thereby revolutionizing the way we work, learn, and communicate in the digital age\"\n",
        "\n",
        "# Stem each word and print the result\n",
        "for word in text:\n",
        "    stemmed_word = porter_stemmer.stem(text)\n",
        "\n",
        "print(stemmed_word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2a983a",
      "metadata": {
        "id": "1d2a983a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}